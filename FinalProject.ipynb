{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinalProject.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahareslami/Feature_Selection/blob/master/FinalProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbIKc9enoGMM",
        "colab_type": "text"
      },
      "source": [
        "# Feature Selection\n",
        "writer : sahar shekholeslami"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rr9T1GDh7EcG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import metrics\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import tree\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import time\n",
        "import pickle\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVOxe_e-Qb78",
        "colab_type": "text"
      },
      "source": [
        "# Downlaod and Save data\n",
        "First, we save all the features of the training data and their labels in one file. and do the same for test data.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNy7rOvHJ7cU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "feea29b6-2687-40de-a310-7431710ef105"
      },
      "source": [
        "!gdown https://drive.google.com/u/0/uc?id=1IhZsc4bRIbnLtAEwAxhHBbaj385_n1ET"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/u/0/uc?id=1IhZsc4bRIbnLtAEwAxhHBbaj385_n1ET\n",
            "To: /content/FinalDataset.zip\n",
            "\r0.00B [00:00, ?B/s]\r9.88MB [00:00, 87.1MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCVTnWeqjiB6",
        "colab_type": "text"
      },
      "source": [
        "simply unizp the file!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoYQjx1eMKpW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip \"/content/FinalDataset.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUIw2JetMaUl",
        "colab_type": "text"
      },
      "source": [
        "The saved format is csv, which is why we use dataframe from pandas for store data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVwT6eiFdmRb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "totalTrainX = pd.read_csv('/content/train/features.csv' , header=None)\n",
        "totalTrainY = pd.read_csv( '/content/train/Labels.csv' , header=None)\n",
        "testX = pd.read_csv('/content/test/features.csv' , header=None)\n",
        "testY = pd.read_csv('/content/test/Labels.csv' , header=None)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2W_vcHtkQSn",
        "colab_type": "text"
      },
      "source": [
        "top_fiveTrainX contains first five data from each category that we want to train our classification algorithm, with in the following\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8M15iJg7TCn-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "top_fiveTrainX = pd.DataFrame()\n",
        "top_fiveTrainY = pd.DataFrame()\n",
        "cntOFData = np.zeros((16), dtype=int)\n",
        "for (index , label) in totalTrainY.iterrows():\n",
        "  if cntOFData[label[0]] < 5:\n",
        "    top_fiveTrainX = top_fiveTrainX.append(totalTrainX.iloc[index])\n",
        "    top_fiveTrainY = top_fiveTrainY.append(totalTrainY.iloc[index])\n",
        "    cntOFData[label[0]] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWruia-flSy9",
        "colab_type": "text"
      },
      "source": [
        "Similarly, top_TenTrainX contains first ten data from each category"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBNnA62xbGg0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "top_TenTrainX = pd.DataFrame()\n",
        "top_TenTrainY = pd.DataFrame()\n",
        "cntOFData2 = np.zeros((16), dtype=int)\n",
        "\n",
        "for (index , label) in totalTrainY.iterrows():\n",
        "  if cntOFData2[label[0]] < 10:\n",
        "    top_TenTrainX = top_TenTrainX.append(totalTrainX.iloc[index])\n",
        "    top_TenTrainY = top_TenTrainY.append(totalTrainY.iloc[index])\n",
        "    cntOFData2[label[0]] += 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a3S-iTCPCgJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5klnxpkDoFjc",
        "colab_type": "text"
      },
      "source": [
        "# Random Forests\n",
        "\n",
        "In random forests , each tree in the ensemble is built from a sample drawn with replacement (i.e., a bootstrap sample) from the training set.\n",
        "\n",
        "Furthermore, when splitting each node during the construction of a tree, the best split is found either from all input features or a random subset of size max_features. (See the parameter tuning guidelines for more details).\n",
        "\n",
        "The purpose of these two sources of randomness is to decrease the variance of the forest estimator. Indeed, individual decision trees typically exhibit high variance and tend to overfit. The injected randomness in forests yield decision trees with somewhat decoupled prediction errors. By taking an average of those predictions, some errors can cancel out. Random forests achieve a reduced variance by combining diverse trees, sometimes at the cost of a slight increase in bias. In practice the variance reduction is often significant hence yielding an overall better model.\n",
        "\n",
        "In contrast to the original publication [B2001], the scikit-learn implementation combines classifiers by averaging their probabilistic prediction, instead of letting each classifier vote for a single class.\n",
        "[Breiman, “Random Forests”, Machine Learning](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKlSJvidnvqx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def randomForestClassifier(tx , ty , vx , vy):\n",
        "  clf = RandomForestClassifier(n_estimators= 500 , max_depth=30, random_state=0 , criterion = \"gini\")\n",
        "  clf.fit(tx, ty)\n",
        "  predY = clf.predict(vx)\n",
        "  return metrics.accuracy_score(vy, predY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSZeoSssUVqS",
        "colab_type": "text"
      },
      "source": [
        "**About Random Forest in Sci_ki learn and Parameters:  \\\\\n",
        "n_estimatorsint:** \\\\\n",
        "The number of trees in the forest. \\\\\n",
        "**criterion :** \\\\\n",
        "The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain. \\\\\n",
        "**max_depth:** \\\\\n",
        "The maximum depth of the tree \\\\\n",
        "**max_features:**  \n",
        "The number of features to consider when looking for the best split \n",
        "\n",
        "1.   If int, then consider max_features features at each split.\n",
        "2.   If float, then max_features is a fraction and int(max_features * n_features) features are considered at each split\n",
        "3. If “auto”, then max_features=sqrt(n_features).\n",
        "\n",
        "4. If “sqrt”, then max_features=sqrt(n_features) (same as “auto”).\n",
        "\n",
        "5. If “log2”, then max_features=log2(n_features).\n",
        "\n",
        "6. If None, then max_features=n_features.\n",
        "\n",
        "**Results**\n",
        "\n",
        "n_estimators | max_depth | criterion | accuracy\n",
        "--- | --- | --- | --- \n",
        "100 |10 | entropy | 0.9642857142857143\n",
        "100 | 30| entropy | 0.9649859943977591 \n",
        "500 | 30 | entropy  | 0.9744397759103641\n",
        "500 | 30 | gini  | 0.979890756302521\n",
        "800 | 30 | entropy | 0.9744397759103641\n",
        "800 | 30 | gini | 0.9793417366946778 \n",
        "\n",
        "The best accuracy is achieved when n_estimatorsint = 500 , max_depth = 30 and criterion = \"gini\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4FpIhCVOA8o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for setting Hyper parameters\n",
        "trainX = totalTrainX.head(250)\n",
        "trainY = totalTrainY.head(250)\n",
        "validationX = totalTrainX.tail(100)\n",
        "validationY = totalTrainY.tail(100)\n",
        "randomForestClassifier(trainX , trainY , validationX , validationY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2gmqIM8aMc6",
        "colab_type": "text"
      },
      "source": [
        "# Multi-layer Perceptron \n",
        "**Multi-layer Perceptron (MLP)** is a supervised learning algorithm that learns a function $f(\\cdot): R^m \\rightarrow R^o$ by training on a dataset, where  is the number of dimensions for input and  is the number of dimensions for output. Given a set of features $X = {x_1, x_2, ..., x_m}$\n",
        "nd a target , it can learn a non-linear function approximator for either classification or regression. It is different from logistic regression, in that between the input and the output layer, there can be one or more non-linear layers, called hidden layers. Figure 1 shows a one hidden layer MLP with scalar output.\n",
        "\n",
        "![MLP](https://doc-0k-5s-docs.googleusercontent.com/docs/securesc/kc1q400hlmlt1l5c0e2qjjelvmpo2scd/rntaop9j60pmlomiqs320j1lj12dq0k2/1594899675000/14271364715593122408/14271364715593122408/1AddtblRmBfSTu0WZeckmsLCf3FSlqEdv?e=download&authuser=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxhPYK1FqDBL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def NNClassifier(tx, ty , vx , vy):\n",
        "  clf = MLPClassifier(random_state=1,hidden_layer_sizes = (100,100) ,  activation = \"logistic\"  , max_iter=500)\n",
        "  clf.fit(tx, ty)\n",
        "  predY = clf.predict(vx)\n",
        "  return metrics.accuracy_score(vy, predY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKo2ObBic9x4",
        "colab_type": "text"
      },
      "source": [
        "**About MLP Classifier in Sci_ki learn and Parameters:** \\\\\n",
        "This model optimizes the log-loss function using LBFGS or stochastic gradient descent. \\\\\n",
        "\n",
        "** hidden_layer_sizes ** \n",
        "The ith element represents the number of neurons in the ith hidden layer. \n",
        "** activation : ** \\\\\n",
        "Activation function for the hidden layer \\\\\n",
        "1. ‘identity’, no-op activation, useful to implement linear bottleneck, returns f(x) = x \\\\\n",
        "2. ‘logistic’, the logistic sigmoid function, returns f(x) = 1 / (1 + exp(-x)).\n",
        "3. ‘tanh’, the hyperbolic tan function, returns f(x) = tanh(x).\n",
        "4. relu’, the rectified linear unit function, returns f(x) = max(0, x)\n",
        "\n",
        "**learning_rate** \\\\\n",
        "Learning rate schedule for weight updates.\n",
        "\n",
        "**Results**\n",
        "\n",
        "hidden_layer_sizes | activation  | accuracy\n",
        "--- | --- | --- \n",
        "(100,100) | relu | 0.91  \n",
        "(100,100) | tanh | 0.95 \n",
        "(100,100) | logistic | 0.96\n",
        "(200,200) | tanh | 0.94\n",
        "(200,200) | relu | 0.94\n",
        "(200,200) | logistic  | 0.96\n",
        "\n",
        "The best accuracy is achieved when hidden_layer_sizes have 100 neurons both and activation function is logistic."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7qLFsC2b5PS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for setting Hyper parameters\n",
        "trainX = totalTrainX.head(250)\n",
        "trainY = totalTrainY.head(250)\n",
        "validationX = totalTrainX.tail(100)\n",
        "validationY = totalTrainY.tail(100)\n",
        "NNClassifier(trainX , trainY , validationX , validationY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tg7i6ifSijgI",
        "colab_type": "text"
      },
      "source": [
        "# Support Vector Machine\n",
        "The objective of the support vector machine algorithm is to find a hyperplane in an N-dimensional space(N — the number of features) that distinctly classifies the data points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0g82FU2vf3vo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def SVMClassifier(tx , ty , vx , vy):\n",
        "  clf = make_pipeline(StandardScaler(), SVC(gamma='auto' , kernel = 'sigmoid'))\n",
        "  clf.fit(tx , ty)\n",
        "  predY = clf.predict(vx)\n",
        "  return metrics.accuracy_score(vy, predY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2z5i1MpJjVSq",
        "colab_type": "text"
      },
      "source": [
        "**About SVM in Sci_kit learn and Parameters: \\\n",
        "kernel :**\n",
        "\n",
        "Specifies the kernel type to be used in the algorithm. It must be one of ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ or a callable. If none is given, ‘rbf’ will be used. If a callable is given it is used to pre-compute the kernel matrix from data matrices.\n",
        "\n",
        "**Results**\n",
        "\n",
        "kernel | accuracy\n",
        "--- | ---  \n",
        "linear | 0.95  \n",
        "poly | 0.1 :(\n",
        "rbf | 0.89\n",
        "sigmoid | 0.94\n",
        "\n",
        "The data are almost linearly separable so using diffrent kerenls doesnot effective!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6aDQ3cnjUgI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for setting Hyper parameters\n",
        "trainX = totalTrainX.head(250)\n",
        "trainY = totalTrainY.head(250)\n",
        "validationX = totalTrainX.tail(100)\n",
        "validationY = totalTrainY.tail(100)\n",
        "print(SVMClassifier(trainX , trainY , validationX , validationY))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-I9mQ1LmRg-",
        "colab_type": "text"
      },
      "source": [
        "# Decision Trees\n",
        "Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qN2Hpi0CzvG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decisionTreeClassifier(tx , ty , vx , vy):\n",
        "  clf = tree.DecisionTreeClassifier()\n",
        "  clf = clf.fit(tx, ty)\n",
        "  predY = clf.predict(vx)\n",
        "  return metrics.accuracy_score(vy, predY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoU0dbXjnqZm",
        "colab_type": "text"
      },
      "source": [
        "This function determines the accuracy of the input features on the train and validation data based on the input's classifier "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uLlcgy_H5x1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_performance(features , labels , approach):\n",
        "  trainX = features.head(250)\n",
        "  trainY = labels.head(250)\n",
        "  validationX = features.tail(100)\n",
        "  validationY = labels.tail(100)\n",
        "  if approach == 'svm':\n",
        "    return SVMClassifier(trainX ,trainY, validationX, validationY)\n",
        "  if approach == 'DT':\n",
        "    return decisionTreeClassifier(trainX ,trainY, validationX, validationY)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTxxOFCfp74z",
        "colab_type": "text"
      },
      "source": [
        "# Forward Feature Selection\n",
        "\n",
        "In this approach, each time we select a feature that is have higher accuracy with the selected features.\n",
        "In the method below,add one feature to selected features in a way that maximizes accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYPKgowyjSZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_One_Feature(selectedFeatures , otherFeatures , selectedIndex):\n",
        "  max_acc , max_idx = 0 , 0\n",
        "  nxt = len(select.columns)\n",
        "  \n",
        "  for (index , feature) in otherFeatures.iteritems():\n",
        "    selectedFeatures.insert(nxt ,nxt ,feature)\n",
        "    cur_acc = find_performance(selectedFeatures ,totalTrainY , 'DT')\n",
        "    if cur_acc > max_acc:\n",
        "      max_acc = cur_acc\n",
        "      max_idx = index\n",
        "    selectedFeatures.drop(nxt , axis=1, inplace=True) \n",
        "\n",
        "  selectedFeatures.insert(nxt ,nxt ,other[max_idx])\n",
        "  otherFeatures = otherFeatures.drop(max_idx , axis=1)\n",
        "  selectedIndex = np.append(selectedIndex , [max_idx])\n",
        "\n",
        "  return selectedFeatures,otherFeatures,selectedIndex"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVqzY46VUO9P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forwardFeatureSelection(select,other,indexs, numberOfFeature):\n",
        "  start_time = time.time()\n",
        "\n",
        "  for i in range(0,numberOfFeature):\n",
        "    select,other,indexs = add_One_Feature(select ,other ,indexs)\n",
        "    \n",
        "  result = pd.DataFrame(indexs)\n",
        "  result.to_csv(r\"/content/forwardFeatureSelection.csv\" ,  index = False, header=True)\n",
        "\n",
        "  with open('/content/forwardFeatureSelection.csv' , 'wb') as handle:\n",
        "    pickle.dump(indexs, handle)\n",
        "  print(\"time\" , time.time() - start_time)\n",
        "  return indexs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBLVKWmnqp3m",
        "colab_type": "text"
      },
      "source": [
        "# Backward Feature Selection\n",
        "In this method, each time we remove a feature that removal of that feature from the set of features has less effect on reducing accuracy.\n",
        "The following two functions remove a feature in such a way that it has maximum accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AI-LUEozj6I0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reduced_feature(select,idx):\n",
        "  select = select.drop(columns = [idx])\n",
        "  return find_performance(select , totalTrainY , \"DT\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60c5cO0Lj0hC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def delete_one_feature(selectedFeatures , selectedIndex):\n",
        "  max_acc , max_idx = 0 , 0\n",
        "  for (index , feature) in selectedFeatures.iteritems():\n",
        "      cur_acc = reduced_feature(selectedFeatures , index)\n",
        "      if cur_acc > max_acc:\n",
        "        max_acc = cur_acc\n",
        "        max_idx = index\n",
        "  selectedFeatures = selectedFeatures.drop(columns=[max_idx])\n",
        "  selectedIndex = np.append(selectedIndex , [max_idx])\n",
        "  return selectedFeatures , selectedIndex"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwtU0L2iKBS1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(trainData):\n",
        "  max_acc , idx = 0 , 0\n",
        "  for (index,feature) in trainData.iteritems():\n",
        "    acc = performanceBaseOnDT(feature.values.reshape(-1,1) , trainY)\n",
        "    if acc > max_acc:\n",
        "      idx = index\n",
        "      max_acc = acc\n",
        "    # print(index , acc)\n",
        "    if acc <= 0.2:\n",
        "      trainData = trainData.drop(columns = [index])\n",
        "  print(\"iki\" , trainData[idx])\n",
        "  return trainData    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKyBS4hQsux8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def backwardFeatureSelection_v1(features , indexs , numberOfFeature):\n",
        "  start_time = time.time()\n",
        "  \n",
        "  features_collection = {} \n",
        "  indexs_collection = {}\n",
        "\n",
        "\n",
        "  for i in range(0,8):\n",
        "    st_index , en_index = i * 256 , (i + 1) * 256\n",
        "    features_collection[i] = pd.DataFrame(features.iloc[:,st_index :  en_index])\n",
        "    indexs_collection[i] =  np.array([], dtype = np.int)\n",
        "\n",
        "\n",
        "  fraction = int((4096 - numberOfFeature) / 8)\n",
        "\n",
        "  for i in range(0 , 8):\n",
        "      for j in range(0 , fraction):\n",
        "        print(j)\n",
        "        features_collection[i] , indexs_collection[i] = delete_one_feature(features_collection[i] ,indexs_collection[i])\n",
        "      indexs = np.append(indexs, indexs_collection[i])\n",
        "\n",
        "  result = pd.DataFrame(indexs)\n",
        "  result.to_csv(r\"/content/backwardFeatureSelectionv2.csv\" ,  index = False, header=True)\n",
        "\n",
        "  with open('/content/backwardFeatureSelectionv2.pickle') as handle:\n",
        "    pickle.dump(indexs, handle)\n",
        "    print(time.time() - start_time)\n",
        "  return features_collection[0],indexs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ysac_2JfrzKK",
        "colab_type": "text"
      },
      "source": [
        "# Ada Boost\n",
        "The core principle of AdaBoost is to fit a sequence of weak learners (i.e., models that are only slightly better than random guessing, such as small decision trees) on repeatedly modified versions of the data. The predictions from all of them are then combined through a weighted majority vote (or sum) to produce the final prediction. The data modifications at each so-called boosting iteration consist of applying weights $w_1,w_2, ..., w_n$ to each of the training samples. Initially, those weights are all set to $w_i = 1/N$ , so that the first step simply trains a weak learner on the original data. For each successive iteration, the sample weights are individually modified and the learning algorithm is reapplied to the reweighted data. At a given step, those training examples that were incorrectly predicted by the boosted model induced at the previous step have their weights increased, whereas the weights are decreased for those that were predicted correctly. As iterations proceed, examples that are difficult to predict receive ever-increasing influence. Each subsequent weak learner is thereby forced to concentrate on the examples that are missed by the previous ones in the sequence\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBVjYE5srxWI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def adaBoost(tx,ty,vx,vy):\n",
        "  start_time = time.time()\n",
        "  clf = AdaBoostClassifier(\n",
        "    tree.DecisionTreeClassifier(max_depth=1),\n",
        "    n_estimators = 800 , learning_rate = 0.1)\n",
        "  clf.fit(tx, ty)\n",
        "  predY = clf.predict(vx)\n",
        "\n",
        "  important_features_dict = {}\n",
        "  for x,i in enumerate(clf.feature_importances_):\n",
        "    important_features_dict[x]=i\n",
        "\n",
        "  important_features_lt = sorted(important_features_dict.items(),key=lambda x: x[1], reverse=True)\n",
        "\n",
        "  indexs = np.array([], dtype = np.int)\n",
        "  for x,i in important_features_lt:\n",
        "    if i > 0.0:\n",
        "      indexs = np.append(indexs, x)\n",
        "\n",
        "  result = pd.DataFrame(indexs)\n",
        "  result.to_csv(r\"/content/adaboost.csv\" ,  index = False, header=True)\n",
        "\n",
        "  with open('adaboost.pickle', 'wb') as handle:\n",
        "    pickle.dump(indexs, handle)\n",
        "\n",
        "  return metrics.accuracy_score(vy, predY) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdmDZ6l1xLZl",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**n_estimatorsint** \\\n",
        "The maximum number of estimators at which boosting is terminated. In case of perfect fit, the learning procedure is stopped early.\n",
        "\n",
        "**learning_ratefloat** \\\n",
        "Learning rate shrinks the contribution of each classifier by learning_rate. There is a trade-off between learning_rate and n_estimators.\n",
        "\n",
        "**Results**\n",
        "\n",
        "n_estimatorsint | learning_ratefloat  | accuracy\n",
        "--- | ---  | ---\n",
        "500 | 0.1  | 0.77\n",
        "500 | 0.2 |  0.73\n",
        "800 | 0.1 |  0.79"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKm8BwoMv79H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "5a3ec790-7727-4600-9748-30ed8f53f6d7"
      },
      "source": [
        "\n",
        "# for setting Hyper parameters\n",
        "trainX = totalTrainX.head(250)\n",
        "trainY = totalTrainY.head(250)\n",
        "validationX = totalTrainX.tail(100)\n",
        "validationY = totalTrainY.tail(100)\n",
        "adaBoost(trainX , trainY , validationX , validationY)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.91"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViU-uyv8rUjd",
        "colab_type": "text"
      },
      "source": [
        "# Train "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z498072GgYp9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_with_selected_feature(indexs , approach):\n",
        "  selected_feature_train_x = totalTrainX.iloc[:,indexs]\n",
        "  selected_feature_test_x = testX.iloc[:,indexs]\n",
        "  if approach == \"RF\":\n",
        "    return randomForestClassifier(selected_feature_train_x , totalTrainY , selected_feature_test_x , testY)\n",
        "  if approach == \"SVM\":\n",
        "     return SVMClassifier(selected_feature_train_x , totalTrainY , selected_feature_test_x , testY) \n",
        "  if approach == \"NN\":\n",
        "     return NNClassifier(selected_feature_train_x , totalTrainY , selected_feature_test_x , testY) \n",
        "  if approach == \"DT\":\n",
        "     return decisionTreeClassifier(selected_feature_train_x , totalTrainY , selected_feature_test_x , testY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbLqQGaI7m8d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_with_five_data(indexs , approach):\n",
        "  selected_feature_train_x = top_fiveTrainX.iloc[:,indexs]\n",
        "  selected_feature_test_x = testX.iloc[:,indexs]\n",
        "  if approach == \"RF\":\n",
        "    return randomForestClassifier(selected_feature_train_x , top_fiveTrainY , selected_feature_test_x , testY)\n",
        "  if approach == \"SVM\":\n",
        "     return SVMClassifier(selected_feature_train_x , top_fiveTrainY , selected_feature_test_x , testY) \n",
        "  if approach == \"NN\":\n",
        "     return NNClassifier(selected_feature_train_x , top_fiveTrainY , selected_feature_test_x , testY) \n",
        "  if approach == \"DT\":\n",
        "     return decisionTreeClassifier(selected_feature_train_x , top_fiveTrainY , selected_feature_test_x , testY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYDkmvE0BcFh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_with_ten_data(indexs , approach):\n",
        "  selected_feature_train_x = top_TenTrainX.iloc[:,indexs]\n",
        "  selected_feature_test_x = testX.iloc[:,indexs]\n",
        "  if approach == \"RF\":\n",
        "    return \n",
        "  if approach == \"SVM\":\n",
        "     return SVMClassifier(selected_feature_train_x ,  top_TenTrainY , selected_feature_test_x , testY) \n",
        "  if approach == \"NN\":\n",
        "     return NNClassifier(selected_feature_train_x ,  top_TenTrainY , selected_feature_test_x , testY) \n",
        "  if approach == \"DT\":\n",
        "     return decisionTreeClassifier(selected_feature_train_x ,  top_TenTrainY , selected_feature_test_x , testY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZofjxsI845rQ",
        "colab_type": "text"
      },
      "source": [
        "# Results\n",
        "Random Forrest algorithm is resistant to the number of data while it is highly vulnerable to the number of features.\n",
        "\n",
        "number of data and the number of features affect the neural network algorithm\n",
        "\n",
        "SVM is also affected by both\n",
        "\n",
        "run the cell below for see the full results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHk2lJvTanpb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_result = pd.read_csv('/content/result.csv' , header=None)\n",
        "final result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFT7g643h6Eh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classfierAlg = np.array([\"SVM\" , \"NN\" , \"RF\"])\n",
        "num_feature = np.array([50 , 100 , 300])\n",
        "all_feature = np.arange(4096)\n",
        "\n",
        "with open('/content/forwardFeaturesSelection.pkl', 'rb') as handle:\n",
        "    Findex = pickle.load(handle)\n",
        "\n",
        "with open('/content/backwardFeatureSelectionv2.pickle', 'rb') as handle:\n",
        "    Bindex = pickle.load(handle)\n",
        "\n",
        "with open('/content/adaboost.pickle', 'rb') as handle:\n",
        "    Aindex = pickle.load(handle)\n",
        "\n",
        "result = pd.DataFrame(columns = ['classifier algorithm', 'number of training data' , 'feature selection algorithm' , 'numebr of feature' , 'accuracy' ])\n",
        "\n",
        "for alg in classfierAlg:\n",
        "  for num in num_feature:\n",
        "    a = train_with_selected_feature(Findex[0:num],alg)\n",
        "    b = train_with_ten_data(Findex[0:num],alg)\n",
        "    c = train_with_five_data(Findex[0:num],alg)\n",
        "    result = result.append({'classifier algorithm' : alg  , 'number of training data' : 5, 'feature selection algorithm' : \"forward\" , 'numebr of feature' : num , 'accuracy' : c} , ignore_index=True)\n",
        "    result = result.append({'classifier algorithm' : alg  , 'number of training data' : 10, 'feature selection algorithm' : 'forward' , 'numebr of feature' : num , 'accuracy' : b} , ignore_index=True)\n",
        "    result = result.append({'classifier algorithm' : alg  , 'number of training data' : 30, 'feature selection algorithm' : \"forward\" , 'numebr of feature' : num , 'accuracy' : a} , ignore_index=True)\n",
        "\n",
        "\n",
        "\n",
        "for alg in classfierAlg:\n",
        "  a = train_with_selected_feature(Aindex,alg)\n",
        "  b = train_with_ten_data(Aindex,alg)\n",
        "  c = train_with_five_data(Aindex,alg)\n",
        "\n",
        "  result = result.append({'classifier algorithm' : alg  , 'number of training data' : 5, 'feature selection algorithm' : \"adaboost\" , 'numebr of feature' : \"-\" , 'accuracy' : c} , ignore_index=True)\n",
        "  result = result.append({'classifier algorithm' : alg  , 'number of training data' : 10, 'feature selection algorithm' : 'adaboost' , 'numebr of feature' : \"-\" , 'accuracy' : b} , ignore_index=True)\n",
        "  result = result.append({'classifier algorithm' : alg  , 'number of training data' : 30, 'feature selection algorithm' : \"adaboost\" , 'numebr of feature' : \"-\" , 'accuracy' : a} , ignore_index=True)\n",
        "\n",
        "\n",
        "bnum_feature = np.array([50 , 100 , 200])\n",
        "\n",
        "for alg in classfierAlg:\n",
        "  for num in bnum_feature:\n",
        "    a = train_with_selected_feature(Bindex[0:num],alg)\n",
        "    b = train_with_ten_data(Bindex[0:num],alg)\n",
        "    c = train_with_five_data(Bindex[0:num],alg)\n",
        "    result = result.append({'classifier algorithm' : alg  , 'number of training data' : 5, 'feature selection algorithm' : \"backward\" , 'numebr of feature' : num , 'accuracy' : c} , ignore_index=True)\n",
        "    result = result.append({'classifier algorithm' : alg  , 'number of training data' : 10, 'feature selection algorithm' : 'backward' , 'numebr of feature' : num , 'accuracy' : b} , ignore_index=True)\n",
        "    result = result.append({'classifier algorithm' : alg  , 'number of training data' : 30, 'feature selection algorithm' : \"backward\" , 'numebr of feature' : num , 'accuracy' : a} , ignore_index=True)\n",
        "\n",
        "\n",
        "for alg in classfierAlg:\n",
        "  a = train_with_selected_feature(all_feature,alg)\n",
        "  b = train_with_ten_data(all_feature,alg)\n",
        "  c = train_with_five_data(all_feature,alg)\n",
        "  result = result.append({'classifier algorithm' : alg  , 'number of training data' : 5, 'feature selection algorithm' : \"all_feature\" , 'numebr of feature' : 4096 , 'accuracy' : c} , ignore_index=True)\n",
        "  result = result.append({'classifier algorithm' : alg  , 'number of training data' : 10, 'feature selection algorithm' : 'all_feature' , 'numebr of feature' : 4096 , 'accuracy' : b} , ignore_index=True)\n",
        "  result = result.append({'classifier algorithm' : alg  , 'number of training data' : 30, 'feature selection algorithm' : \"all_feature\" , 'numebr of feature' : 4096 , 'accuracy' : a} , ignore_index=True)\n",
        "\n",
        "\n",
        "result.to_csv(r\"/content/result.csv\" ,  index = False, header=True)\n",
        "with open('result.pickle', 'wb') as handle:\n",
        "    pickle.dump(result, handle)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}